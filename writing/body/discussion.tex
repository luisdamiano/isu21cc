\chapter{Discussion}

\OUTLINE{Highlights about the algorithm} We have proposed an algorithm
for data processing and visualization that is designed for situations
where the destructive sampling scheme incorporates other non-trivial
features, namely time order and unequally-sized, irregularly-shaped,
and possibly overlapping areal observational units. Reproducing the
sampling process, the rules unpack the georeferenced coordinates,
recorded as points in a 2-dimensional space, into polygons that are
next tiled. The resulting geometries are aggregated using basic areal
units and apportioned assuming that the variable of interest follows a
uniform distribution at the local scale. The final output of the
processing rules is in the form of a regular tessellation of the
harvested area, equipped with both their associated observed and
smoothed yield; the output is meant for both data analysis and
visualization. As a byproduct, because the location of the pixels in
both the aggregation and smoothing grids can be chosen freely without
affecting the validity of the algorithm, spatial registration of the
output is readily available.

\OUTLINE{Comparison with previous work} To illustrate the main
characteristics of our procedure, we have presented a case study of
grain yield data processing and visualization. Whereas the previous
work in precision agriculture presents an almost unanimous preference
for extreme values detection and removal, with up to 32\% of the data
excluded \cite{Lyle2013}, our physically principled approach generates
a smooth visualization while retaining as much information as
collected. This improvement is partly explained by our reproducing of
the sampling scheme, but also by a twist on how the problem has been
traditionally framed: recognizing that local yield is a highly
volatile measurement unit due to scaling and error propagation, and
thus effectively shifting the baseline variable from yield to
mass. Also in contrast with previously proposed methodologies, most of
which require the map producer to set arbitrary thresholds, our
algorithm is autonomous. There are two main advantages associated with
this: (i) large amount of grain yield datasets can be processed
automatically, a relevant feature in the current times where data
collection has became accessible and widespread even at a small
farming scale; and (ii) the final outputs become more consistent
across different users, fields, and years.

% \OUTLINE{Our contribution (3) Implementation} Our contributions also
% includes an open-source implementation including both the processing
% rules as well as the visualization maps. The R package
% \texttt{yieldMap} provides tools to run this algorithm on the dataset
% used for the case study, which is provided along with the package as
% well, or simply on custom data. Besides providing some flexbility in
% terms of visualization options, it exploits parallelization when
% computing the smoothed yield via the Gaussian Process.

When doing exact inference for a Gaussian Process, smoothing requires
the inversion of a square matrix with size equal to the number of
observations in the aggregation step, which in this case study is
3,738 pixels. Unless resorting to approximate interpolation methods,
significant time is expected to be consumed for this purpose. As a
palliative, our implementation optionally divides the prediction space
into smaller subsets to be computed in parallel. Although there is no
direct gain in the matrix inversion time, it provides with some
marginal improvements as out prediction space is large with more than
90,000 pixels.

\OUTLINE{Future research: assumptions} Future improvements of this
algorithm could come from three perspectives: assumptions,
visualization, and computations. One of the main assumptions is that,
at a local scale, the random variable of interest follows a uniform
distribution. Although this is a reasonable idealization for the case
study given the small area of the observational units relative to the
rate of change of the underlying process, applications for
highly-distanced observations or rapidly-changing underlying processes
may require the extension to a different distribution
(e.g. exponential decay from the centroids of the polygons to its
boundaries). Yield monitors may record additional variables that are
currently not exploited by our algorithm, which could be enhanced for
example by smoothing via universal kriging with automatic relevance
determination for feature selection.

\OUTLINE{Future research: visualization} To improve visualizations,
the smooth map could profit from better display techniques to signal
the contrast between low and high valued areas (e.g. contour lines,
spatial clustering techniques). In the specific case of grain yield
maps, we still observe that some of the well-known sources of yield
data error transpire through the algorithm. Some of these can be
treated previously to running the algorithm, for example the time lag
effect, the harvester fill mode error, and position errors as
described in \cite{Blackmore1999}.

\OUTLINE{Future research: computation} On the computational side, the
case study suggests that the most time consuming steps are smoothing
and tiling, respectively. The former could be reduced by resourcing to
approximation methods for Gaussian Process spatial interpolation
\citep{Shi2007,Cressie2008,Katzfuss2011,Nguyen2012,Nguyen2014},
approximate linear algebra routines, or a more efficient interpolation
technique. Although the intrisic sequential nature of the sampling
scheme limits the potential of parallelization in the tiling step,
some performance improvements could be potentially achieved by
subviding the spatial domain into disjoint blocks that should after be
accordingly recoupled (divide and conquer). Alternatively, the bitmap
matrix of \cite{Han1997} could be revisited as an approximation to our
polygon approach. These gains could turn the current work into a near
real-time algorithm.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
